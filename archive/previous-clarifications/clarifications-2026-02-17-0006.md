## Clarifications Needed

The following items were identified during a comprehensive architecture review of all spec documents (v1.8â€“v2.3) after the recent updates adding link click tracking, time-on-page milestones, visitor session ID, and the removal of Firestore tracking/error storage (DM-007/DM-008).

---

### CLR-050: Embedding Task Type for Search Query Embedding ðŸŸ¡

**Context**: BE-API-002 Vector Search Flow step 3 specifies calling Vertex AI Gemini `gemini-embedding-001` with `task_type=RETRIEVAL_DOCUMENT` when embedding a user's search query at runtime. However, Google's documentation for Gemini embedding models recommends using different task types for queries vs documents:

- `RETRIEVAL_QUERY` â€” optimized for the **query** side of a retrieval task (what the user searches for)
- `RETRIEVAL_DOCUMENT` â€” optimized for the **document** side (the content being searched)

Using mismatched task types can reduce search quality because the model optimizes the embedding space differently for each role. INFRA-014 and DM-012 correctly use `RETRIEVAL_DOCUMENT` for article embeddings (the document side).

**Reference**: https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#task-types

**Question**: Should the runtime search query embedding (BE-API-002 step 3) use `task_type=RETRIEVAL_QUERY` instead of `RETRIEVAL_DOCUMENT`?

**Options**:

- **A**: Use `RETRIEVAL_QUERY` for search query embeddings at runtime. *(Recommended â€” per Google's documentation)*
- **B**: Keep `RETRIEVAL_DOCUMENT` for both queries and documents.

**Impact**: Affects BE-API-002 step 3. If changed, existing entries in the `embedding_cache` (DM-011) would need to be invalidated (cleared) since they were generated with the wrong task type. This is a one-time cache clear operation.

**Answer**:
Use A.

---

### CLR-051: Cloud Armor Load Balancer Logs Contain Full IP Addresses ðŸ”´

**Context**: SEC-007 states: "THE SYSTEM SHALL NOT store full IP addresses anywhere in the system." The Go backend truncates IPs before emitting structured logs (which flow to BigQuery). However, the `cloud_armor_lb_logs` BigQuery table (INFRA-010b) receives logs generated by Google's infrastructure at the load balancer level â€” **before** the request reaches the Go backend. These logs include `httpRequest.remoteIp` with the **full** client IP address.

INFRA-010 states these logs have "IP last octet zeroed by Cloud Armor config," but Cloud Armor does not have a native feature to truncate IPs in its exported log entries. The `httpRequest` fields in load balancer logs are populated by Google's infrastructure, not the application. This means full IP addresses are stored in the `cloud_armor_lb_logs` BigQuery table, violating SEC-007.

**Question**: How should IP privacy be handled for Cloud Armor load balancer logs exported to BigQuery?

**Options**:

- **A**: Accept that infrastructure-level LB logs contain full IPs and update SEC-007 and INFRA-010 to acknowledge this exception. Only the website owner has access to this BigQuery table. Note that these logs are essential for DDoS analysis and security incident response, where full IPs are necessary.
- **B**: Create a BigQuery scheduled query or authorized view that masks IPs after ingestion (truncate last octet). Looker Studio accesses the masked view instead of the raw table. The raw table still contains full IPs but is accessed only for security investigations.
- **C**: Remove the `cloud_armor_lb_logs` BigQuery log sink entirely. Cloud Armor metrics are still available in Cloud Monitoring for operational alerting; only the long-term BigQuery analytics are lost.

**Impact**: Affects SEC-007 (privacy compliance statement), INFRA-010b (BigQuery table description), and the privacy policy (FE-PAGE-009). If full IPs are retained in any form, the privacy policy may need to disclose this.

**Answer**:
- **A**: Accept that infrastructure-level LB logs contain full IPs and update SEC-007 and INFRA-010 to acknowledge this exception. Only the website owner has access to this BigQuery table. Note that these logs are essential for DDoS analysis and security incident response, where full IPs are necessary. Let's put Cloud Armor logs in a different table in Bigquery with a table of TTL 90days and update Privacy log for transparency that full IP address is logged for DDoS analysis and security incident response but for Looker Studio, it will not show up for the reports.

---

### CLR-052: DM-009 Rate Limit Offenders Cleanup Mechanism Undefined ðŸŸ¡

**Context**: DM-009 specifies a retention policy: "remove offense records where there is no active ban and no offenses in the last 90 days." It states: "A scheduled cleanup (e.g., via a Cloud Scheduler job or application-level TTL check) SHALL enforce this retention policy." However, unlike the sitemap generation (INFRA-008a/008b) and rate limit log processing (INFRA-008c), **no concrete INFRA component** is defined for this cleanup. There is no Cloud Scheduler job, Cloud Function, or TTL mechanism specified to perform the periodic deletion of expired offender records.

**Question**: How should the DM-009 cleanup be implemented?

**Options**:

- **A**: Add a new Cloud Function + Cloud Scheduler job (e.g., `cleanup-rate-limit-offenders`, runs daily) that queries `rate_limit_offenders` for expired records and deletes them. Similar pattern to INFRA-008a/008b. *(Recommended)*
- **B**: Implement lazy cleanup in the Go backend â€” when checking ban status for an IP, also check if the record is expired and delete it if so. Simpler but may leave orphaned records for IPs that never return.
- **C**: Use Firestore TTL policy (if supported in Enterprise/MongoDB compat mode) to auto-expire documents after 90 days of no ban.
- **D**: Manual cleanup by the website owner periodically.

**Impact**: If A is selected, it adds a new Cloud Function to the infrastructure (affects spec 01 Technology Stack, spec 05 INFRA components, and the cost estimate). If B, it adds complexity to the Go backend.

**Answer**:
Use A

---

### CLR-053: SEC-007 Stale Firestore TTL Reference for Tracking Data ðŸŸ¢

**Context**: SEC-007 Privacy Compliance section contains: _"Tracking and error report data SHALL have TTL-based auto-expiry in Firestore."_ This references the removed DM-007 (`tracking`) and DM-008 (`error_reports`) Firestore collections. Tracking and error data is no longer stored in Firestore â€” it flows exclusively to BigQuery via structured logs and log sinks (INFRA-010). The BigQuery retention (2-year table expiry) is already documented in INFRA-010.

Additionally, the privacy policy disclosure bullet says: _"Retention periods for all data stores (Firestore and BigQuery)"_ â€” which is misleading since tracking data is now BigQuery-only.

**Question**: Should the stale Firestore TTL reference be removed and replaced?

**Options**:

- **A**: Remove the stale line and replace with: _"Tracking and error report data is retained in BigQuery for up to 2 years via dataset-level table expiry (see INFRA-010). No tracking data is stored in Firestore."_ Also update the privacy policy disclosure bullet to say _"Retention periods for all data stores"_ without specifically enumerating Firestore for tracking data. *(Recommended)*
- **B**: Remove the stale line without replacement (BigQuery retention is already documented in INFRA-010).

**Impact**: Affects SEC-007 only. No functional change.

**Answer**:
Use A.

---

### CLR-054: OBS-001 Log Schema Example Shows Full IP Address ðŸŸ¢

**Context**: The example log schema in OBS-001 shows `"client_ip": "203.0.113.42"` â€” a full IP address. Per SEC-007, IPs must be truncated (last octet zeroed for IPv4) before the Go backend emits structured log entries. The example should reflect the truncated format to be consistent with the specification.

**Question**: Should the OBS-001 example be updated to show a truncated IP?

**Options**:

- **A**: Yes, update to `"client_ip": "203.0.113.0"` in the example log schema. *(Recommended)*
- **B**: Keep as-is â€” it's an illustrative example and not normative.

**Impact**: OBS-001 only. Documentation accuracy.

**Answer**:
Use A.

---

### CLR-055: Spec 02 Definitions Table Missing New Tracking Concepts ðŸŸ¢

**Context**: The Definitions table in spec 02 (Frontend Specifications) was not updated when `visitor_session_id`, `visitor_id`, link click tracking, and time-on-page milestones were added. These are important new concepts referenced throughout FE-COMP-004, BE-API-009, and OBS-002.

**Suggested terms to add**:

| Term | Definition |
| --- | --- |
| `visitor_session_id` | A random UUID v4 generated per browser session, stored in `sessionStorage`. Used to correlate tracking events within a single visit. Not linked to any real-world identity and not persisted across sessions. |
| `visitor_id` | A SHA-256 hash computed server-side from `visitor_session_id` + truncated IP + User-Agent. A non-reversible, session-scoped identifier for unique visitor counting and bot discrimination. |
| Link Click Tracking | Fire-and-forget anonymous tracking of anchor element clicks, sent via `navigator.sendBeacon()` to `POST /t`. |
| Time-on-Page Milestone | An engagement signal sent when a user remains on a page for 1, 2, or 5 minutes of active foreground time. Only foreground time counts (uses Page Visibility API). |

**Question**: Should these terms be added to the spec 02 Definitions table?

**Options**:

- **A**: Yes, add all 4 terms as shown above. *(Recommended)*
- **B**: Add only `visitor_session_id` and `visitor_id` (the tracking terms are self-explanatory in FE-COMP-004 context).
- **C**: No changes needed.

**Impact**: Spec 02 Definitions table only. Documentation clarity.

**Answer**:
Use A.

---

## Architecture Review Notes (Non-Blocking)

The following observations were noted during the review. They do not require clarification but are documented for awareness:

### Note 1: Cost Estimate Requires Significant Update

The cost estimate (`docs/cost-estimate-draft.md` v0.1-draft) has several items that are out of sync with the current specs:

- **Cloud Run memory**: Cost estimate uses 256 MB; INFRA-003 specifies 1 GB.
- **Firestore tracking writes**: Cost estimate includes ~3,000/month writes for tracking; DM-007/DM-008 are removed (tracking goes to BigQuery only).
- **POST /t volume**: With new link_click and time_on_page events, the tracking call volume is approximately 2Ã— the page view count, not 1Ã— as currently modeled.
- **Missing services**: BigQuery (storage/query), Vertex AI (embedding API), Firestore Native (vector search), Cloud Functions (3 functions), and VPC Serverless connector (production only) are not included.
- **Cloud Armor rules**: INFRA-005 defines 7 rules (including default allow); cost estimate uses 6.

**Action**: The cost estimate will be updated alongside this clarification document.

### Note 2: Per-Request Firestore Read for Ban Status

SEC-002 specifies that the Go backend checks `rate_limit_offenders` (DM-009) on **every incoming request**. At normal traffic (12,000 requests/month), this adds 12,000 Firestore reads/month. At spike traffic (600,000+ requests/month), this adds 600,000 reads/month. The cost impact is negligible ($0.004â€“$0.22/month at Firestore standard pricing), but the latency impact (~5-10ms per read in the same region) is worth noting for cold-start scenarios.

### Note 3: VPC Serverless Connector Cost

INFRA-009 specifies a VPC with a Serverless VPC Access Connector for production. Connectors require a minimum of 2 e2-micro instances running continuously (~$12/month). As an alternative, Cloud Run and Cloud Functions Gen 2 both support **Direct VPC Egress** (GA since 2023), which provides VPC connectivity without a separate connector at no additional cost. This could save ~$12/month.

### Note 4: Embedding Sync Function Missing from Spec 01 Technology Stack

The Technology Stack table in spec 01 lists "Sitemap Gen" and "Log Processing" as Cloud Functions but does not include the "Embedding Sync" Cloud Function (INFRA-014, `sync-article-embeddings`). It should be added for completeness.

## Addtional Updates from Me:
- Let's use Direct to VPC egress: https://docs.cloud.google.com/run/docs/configuring/shared-vpc-direct-vpc instead of VPC Connectors to lessen cost 
- Understand the IP Address Allocation: https://docs.cloud.google.com/run/docs/configuring/shared-vpc-direct-vpc#direct-vpc-ip-allocation so as to set the subnet mask for our asia-southeast1 for Cloud Run, and also set this as the setting for our Cloud functions.